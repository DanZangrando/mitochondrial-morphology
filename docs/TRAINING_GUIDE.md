# Gu√≠a de Entrenamiento - VAE con TensorBoard en Tiempo Real

## üéì Nueva P√°gina de Entrenamiento

Se ha agregado una nueva p√°gina dedicada al entrenamiento de modelos VAE con monitoreo en tiempo real.

### Acceso

1. Ejecuta la aplicaci√≥n: `streamlit run app.py`
2. En el men√∫ lateral, selecciona: **üéì Entrenar Modelo**

## üöÄ Caracter√≠sticas

### Entrenamiento Interactivo

- **Selecci√≥n de Modelo**:
  - üìä VAE Est√°ndar (mean pooling)
  - üî¨ LSTM-VAE (preserva secuencias)

- **Configuraci√≥n de Hiperpar√°metros**:
  - Max Epochs: 10-500 (default: 100)
  - Learning Rate: 0.00001-0.01 (default: 0.001)
  - Batch Size: 2-32 (auto: 16 para VAE, 4 para LSTM-VAE)
  - Early Stopping Patience: 5-50 (default: 15)

### TensorBoard Embebido

Durante el entrenamiento, TensorBoard se ejecuta autom√°ticamente y se muestra en un iframe dentro de la aplicaci√≥n:

- **M√©tricas en Tiempo Real**:
  - Loss (train/validation)
  - Accuracy (train/validation)
  - Learning rate
  - KL divergence
  - Reconstruction loss
  - Classification loss

- **Visualizaciones**:
  - Curvas de aprendizaje
  - Histogramas de pesos
  - Distribuci√≥n de gradientes
  - Gr√°fico de arquitectura

### Flujo de Trabajo

1. **Seleccionar Tipo de Modelo**
   - Usa VAE est√°ndar para an√°lisis r√°pido
   - Usa LSTM-VAE para capturar variabilidad intra-participante

2. **Configurar Hiperpar√°metros**
   - Ajusta seg√∫n tus necesidades
   - Los valores por defecto son un buen punto de partida

3. **Iniciar Entrenamiento**
   - Click en "üöÄ Iniciar Entrenamiento"
   - TensorBoard se inicia autom√°ticamente
   - Monitorea el progreso en tiempo real

4. **Evaluar Resultados**
   - Una vez completado, ve a **ü§ñ Autoencoder**
   - Carga el modelo reci√©n entrenado
   - Visualiza espacio latente y m√©tricas

## üìä Comparaci√≥n de Modelos

### VAE Est√°ndar

**Caracter√≠sticas**:
- Input: 8 features agregadas (mean pooling)
- Encoder: [64, 32] ‚Üí Latent 8D
- Decoder: [32, 64] ‚Üí Output 8
- Classifier: [16] ‚Üí 2 clases
- Par√°metros: ~6,700

**Ventajas**:
- Entrenamiento r√°pido (~2-5 min)
- Menos memoria
- Interpretable
- Buen baseline

**Desventajas**:
- Pierde variabilidad intra-participante
- Capacidad representacional limitada

### LSTM-VAE

**Caracter√≠sticas**:
- Input: Secuencias variables (4-36 mediciones)
- Encoder: Bidirectional LSTM (2 capas, hidden=64)
- Latent: 16D (desde hidden state)
- Decoder: Unidirectional LSTM (2 capas, hidden=64)
- Classifier: [32, 16] ‚Üí 2 clases
- Par√°metros: ~205,850

**Ventajas**:
- Preserva variabilidad intra-participante
- Mayor capacidad representacional
- Captura patrones temporales/secuenciales
- Puede revelar informaci√≥n oculta

**Desventajas**:
- Entrenamiento m√°s lento (~5-15 min)
- M√°s memoria
- M√°s par√°metros ‚Üí mayor riesgo de overfitting

## üîß Soluci√≥n de Problemas

### TensorBoard No Se Muestra

**S√≠ntoma**: El iframe de TensorBoard aparece vac√≠o

**Soluciones**:
1. Espera 3-5 segundos para que TensorBoard inicie
2. Refresca la p√°gina de Streamlit
3. Verifica que el puerto 6006 no est√© en uso:
   ```bash
   lsof -i :6006
   pkill -f tensorboard
   ```
4. Inicia TensorBoard manualmente:
   ```bash
   tensorboard --logdir=logs/
   ```

### Error Durante el Entrenamiento

**S√≠ntoma**: El entrenamiento falla con un error

**Soluciones**:
1. Verifica las dependencias:
   ```bash
   pip install -r requirements.txt
   ```

2. Verifica el dataset:
   ```bash
   python -c "import pandas as pd; print(pd.read_csv('data/data.csv').shape)"
   ```

3. Entrena manualmente para ver el error completo:
   ```bash
   # VAE est√°ndar
   python scripts/train_autoencoder.py
   
   # LSTM-VAE
   python scripts/train_autoencoder.py --lstm
   ```

4. Revisa los logs de PyTorch Lightning en `logs/`

### Memoria Insuficiente

**S√≠ntoma**: CUDA out of memory o proceso muerto

**Soluciones**:
1. Reduce el batch size
2. Entrena en CPU (m√°s lento pero funciona):
   - Edita `scripts/train_autoencoder.py`
   - Cambia `accelerator='auto'` a `accelerator='cpu'`
3. Reduce el n√∫mero de capas LSTM o hidden_dim

## üìà Interpretaci√≥n de M√©tricas

### Loss Total
- **Baja r√°pidamente**: Modelo aprende bien
- **Plateau temprano**: Aumenta learning rate o complejidad
- **Oscila mucho**: Reduce learning rate o batch size

### Accuracy
- **>70%**: Muy bueno para este dataset
- **50-70%**: Aceptable, features tienen algo de poder discriminativo
- **~50%**: Malo, modelo no aprende (random guessing)

### Val Loss vs Train Loss
- **Similares**: Buen balance, no overfitting
- **Val > Train (moderado)**: Normal, algo de overfitting
- **Val >> Train**: Overfitting severo, reduce complejidad o agrega dropout

### Reconstruction Loss
- **Baja**: Modelo reconstruye bien
- **Alta**: Modelo pierde informaci√≥n
- **Muy baja**: Posible overfitting

### KL Divergence
- **Muy baja**: Espacio latente no est√° regularizado
- **Muy alta**: Espacio latente muy comprimido, pierde informaci√≥n
- **Balance**: ~0.01-0.1 para VAE, ~0.001-0.01 para LSTM-VAE

## üí° Tips de Entrenamiento

### Para Mejores Resultados

1. **Prueba ambos modelos**: Compara VAE vs LSTM-VAE
2. **Usa early stopping**: Deja patience=15-20 para evitar overfitting
3. **Monitorea val_acc**: Es la m√©trica m√°s importante
4. **Experimenta con KL weight**: Afecta estructura del espacio latente
5. **Limpia logs**: Entre entrenamientos para comparaciones limpias

### Ajuste de Hiperpar√°metros

**Si val_loss no baja**:
- Aumenta learning rate (0.001 ‚Üí 0.003)
- Aumenta max_epochs (100 ‚Üí 200)
- Aumenta complejidad del modelo

**Si overfitting (val_loss sube)**:
- Aumenta dropout (0.2 ‚Üí 0.4)
- Reduce complejidad
- Aumenta KL weight
- Agrega m√°s regularizaci√≥n

**Si entrenamiento muy lento**:
- Aumenta batch size (4 ‚Üí 8)
- Reduce max_epochs
- Usa VAE est√°ndar en lugar de LSTM

## üîÑ Workflow Completo

```mermaid
graph TD
    A[Inicio] --> B[P√°gina Entrenar Modelo]
    B --> C{Tipo de Modelo}
    C -->|VAE Est√°ndar| D[Configurar Hiperpar√°metros]
    C -->|LSTM-VAE| D
    D --> E[Iniciar Entrenamiento]
    E --> F[TensorBoard en Tiempo Real]
    F --> G{Entrenamiento OK?}
    G -->|S√≠| H[Modelo Guardado]
    G -->|No| I[Ajustar Hiperpar√°metros]
    I --> E
    H --> J[P√°gina Autoencoder]
    J --> K[Cargar Modelo]
    K --> L[Visualizar Resultados]
    L --> M{Satisfecho?}
    M -->|No| N[Entrenar Nuevo Modelo]
    N --> B
    M -->|S√≠| O[An√°lisis Completo]
```

## üìö Referencias

- [TensorBoard Documentation](https://www.tensorflow.org/tensorboard)
- [PyTorch Lightning Logging](https://lightning.ai/docs/pytorch/stable/extensions/logging.html)
- [Streamlit Components](https://docs.streamlit.io/library/components)
- [VAE Tutorial](https://arxiv.org/abs/1312.6114)
- [LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## ü§ù Contribuciones

Si encuentras bugs o tienes sugerencias:
1. Abre un issue en GitHub
2. Describe el problema con capturas de pantalla
3. Incluye logs relevantes de la terminal o TensorBoard
